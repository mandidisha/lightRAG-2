
# ğŸ§  LightRAG: Explainable QA with Hybrid Retrieval and Knowledge Graphs

**LightRAG** is a research-grade Retrieval-Augmented Generation (RAG) system that enhances factual QA using:

- ğŸ”„ **Hybrid retrieval** (dense + knowledge graph)
- ğŸ“– **Explainable answer generation**
- ğŸ“Š **Quantitative + statistical evaluation**

This implementation supports a Masterâ€™s thesis focused on **trust and explainability in generative AI systems**.

---

## ğŸ” Motivation

As LLMs grow in popularity, the need for **transparency** and **grounded QA** becomes essential.

LightRAG aims to:
- Improve factual accuracy with **structured triples**
- Reduce hallucination via **hybrid evidence grounding**
- Evaluate retrieval and generation using **standard metrics** + **statistical tests**

---

## ğŸ“¦ Project Structure

```
.
â”œâ”€â”€ main.py                   # Pipeline + evaluation + UI launcher
â”œâ”€â”€ constructKG.py           # Hybrid triple extraction (spaCy + LLM)
â”œâ”€â”€ knowledge_graph.py       # Lightweight triple-based KG class
â”œâ”€â”€ retriever_pipeline.py    # End-to-end KG building pipeline
â”œâ”€â”€ dataset.py               # Loads Natural Questions subset
â”œâ”€â”€ metrics_utils.py         # EM, F1, Recall@K, hallucination, tests
â”œâ”€â”€ README.md                # You're here!
â””â”€â”€ requirements.txt         # Dependencies
```

---

## ğŸ”§ System Components

### 1ï¸âƒ£ Triple Extraction

| Method | Description |
|--------|-------------|
| `extract_triples_spacy` | Uses spaCy NER and rules |
| `extract_triples_llm`   | Uses Mistral (Ollama) for semantic parsing |
| `hybrid_triple_extraction` | Switches between both |

---

### 2ï¸âƒ£ Retrieval Modules

- `KGRetriever` â€“ FAISS + CrossEncoder reranking on triple graph
- `DenseRetriever` â€“ MiniLM dense passage retriever
- `HybridRetriever` â€“ Combines both, filters duplicates

---

### 3ï¸âƒ£ Answer Generation

| Model        | Role                 |
|--------------|----------------------|
| **Flan-T5**  | Evaluation only (EM/F1 scoring) |
| **Mistral**  | UI answer display via Ollama  |

---

### 4ï¸âƒ£ Metrics & Testing

- **Recall@5** â€“ Does gold answer appear in retrieved context?
- **Exact Match (EM)** â€“ String match (normalized)
- **F1 Score** â€“ Token-level match
- **Hallucination Rate** â€“ 50%+ hallucinated answer = fail
- **Significance Tests**:
  - Paired t-tests (F1/EM)
  - Wilcoxon tests (non-parametric)
  - Chi-squared (hallucination frequency)

---

## ğŸš€ How to Run

### 1. Install Dependencies

```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

> âš ï¸ You must have Ollama installed and running locally with `mistral` pulled:
> https://ollama.com/library/mistral

---

### 2. Run with spaCy extraction (fast)

```bash
python main.py
```

### 3. Run with LLM extraction (more accurate)

```bash
python main.py --use_llm
```

---

## ğŸ§ª Evaluation Example

```
=== Retrieval Recall@5 ===
KG Recall@5:  39.50%
Dense Recall@5:  99.00%
Hybrid Recall@5:  20.88%

=== Answer EM / F1 ===
KG EM: 26.00%  F1: 41.00%
Dense EM: 35.50%  F1: 57.08%
Hybrid EM: 34.00%  F1: 53.10%

=== Hallucination Rate (>50%) ===
KG: 10.00%
Dense: 0.50%
Hybrid: 0.50%

=== Statistical Significance Tests ===
KG vs Dense â†’ F1 p=0.0002 âœ…
KG vs Hybrid â†’ F1 p=0.0009 âœ…
Dense vs Hybrid â†’ F1 p=0.6709 âŒ
```

---

## ğŸ§ª Research Questions Supported

- **RQ1**: Can hybrid retrieval improve factual accuracy over dense-only methods?
- **RQ2**: Does KG-based grounding reduce hallucination?
- **RQ3**: Are model differences statistically significant?

---

## ğŸ“š Dataset

- Uses: [Natural Questions - Short Form](https://huggingface.co/datasets/cjlovering/natural-questions-short)
- Limit: Subset of 100â€“300 samples for efficient QA evaluation

---

## ğŸ› Gradio UI (Real-time Demo)

- Enter your own questions
- See answers from:
  - KG-based retrieval (Mistral)
  - Dense retrieval (Mistral)
  - Hybrid combined (Mistral)
- Compare retrieved facts per model

---

## ğŸ“˜ Academic References

- Lewis et al., 2020. *Retrieval-Augmented Generation*. NeurIPS.
- Petroni et al., 2019. *Language Models as Knowledge Bases?*. EMNLP.
- Guu et al., 2020. *REALM: Retrieval-Augmented Language Model Pre-Training*.
- Rasooli et al., 2021. *Natural Questions: Benchmarking QA*. Google Research.

---

## ğŸ“„ License

MIT License â€” free for academic and research use.
